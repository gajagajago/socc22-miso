{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e6e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pdb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from unet_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8599f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"padding_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 3, 7, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 4, 8, 1)      0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 8, 8)      40          zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 8, 8)      32          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 8, 8)      0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 8, 8)      264         activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 4, 8, 8)      32          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 8, 8)      0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 2, 4, 8)      0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 2, 4, 16)     528         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 4, 16)     64          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 2, 4, 16)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 2, 4, 16)     1040        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 2, 4, 16)     64          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 2, 4, 16)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 2, 16)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 1, 2, 64)     4160        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1, 2, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 1, 2, 64)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 1, 2, 64)     16448       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 1, 2, 64)     256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 1, 2, 64)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 2, 4, 16)     4112        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 2, 4, 32)     0           activation_39[0][0]              \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2, 4, 32)     128         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 4, 32)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 2, 4, 16)     2064        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 2, 4, 16)     64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 4, 16)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 2, 4, 16)     1040        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 2, 4, 16)     64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 4, 16)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 4, 8, 8)      520         activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 8, 16)     0           activation_37[0][0]              \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 8, 16)     64          concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 8, 16)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 8, 8)      520         activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 8, 8)      32          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 8, 8)      0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 8, 8)      264         activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 8, 8)      32          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 8, 8)      0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Conv2D)           (None, 4, 8, 1)      9           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 3, 7, 1)      0           output_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 32,097\n",
      "Trainable params: 31,553\n",
      "Non-trainable params: 544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet = padding_model() \n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580dc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'start_neurons'   :8,      # Controls size of hidden layers in CNN, higher = more complexity \n",
    "    'activation'      :'relu',  # Activation used throughout the U-Net,  see https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "    'loss'            :'mae',   # Either 'mae' or 'mse', or others as https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "    'loss_weights'    : 0.021,    # Scale for loss.  Recommend squaring this if using MSE\n",
    "    'opt'             :tf.keras.optimizers.Adam,  # optimizer, see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    'learning_rate'   :0.001,   # Learning rate for optimizer\n",
    "    'num_epochs'      :200,       # Number of epochs to train for\n",
    "    'batch'           :8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99af6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=params['opt'](learning_rate=params['learning_rate'])\n",
    "unet.compile(optimizer=opt, loss=params['loss'],loss_weights=[params['loss_weights']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df342f36",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b025a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 18:27:53.867695: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-24 18:27:54.326909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38428 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:25:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.load('input_data.npy')\n",
    "X_data = np.expand_dims(X_data, axis=-1)\n",
    "Y_data = np.load('target_data.npy')\n",
    "Y_data = np.expand_dims(Y_data, axis=-1)\n",
    "\n",
    "tensorslice = tf.data.Dataset.from_tensor_slices((X_data,Y_data)).shuffle(buffer_size=len(Y_data),reshuffle_each_iteration=True).batch(params['batch'])\n",
    "train_data = tensorslice.skip(int(len(tensorslice) * 0.25))\n",
    "test_data = tensorslice.take(int(len(tensorslice) * 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc7778",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8d7dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:36:57.702352: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 12:36:57.702413: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 12:36:57.702545: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:36:57.702619: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 12:36:57.702648: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   3/1313 [..............................] - ETA: 2:03 - loss: 0.0134 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:36:58.687557: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 12:36:58.687604: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 12:36:58.687679: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:36:58.801457: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-24 12:36:58.801569: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:36:58.804353: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-02-24 12:36:58.805292: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 12:36:58.808995: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58\n",
      "\n",
      "2022-02-24 12:36:58.811589: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.trace.json.gz\n",
      "2022-02-24 12:36:58.822814: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58\n",
      "\n",
      "2022-02-24 12:36:58.827383: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.memory_profile.json.gz\n",
      "2022-02-24 12:36:58.836047: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58\n",
      "Dumped tool data for xplane.pb to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.xplane.pb\n",
      "Dumped tool data for overview_page.pb to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to training_dir/tensorboard/padding_unet/train/plugins/profile/2022_02_24_12_36_58/d3103.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 9s 6ms/step - loss: 0.0013 - val_loss: 8.3806e-04\n",
      "Epoch 2/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 8.0804e-04 - val_loss: 8.1306e-04\n",
      "Epoch 3/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 7.2784e-04 - val_loss: 6.7216e-04\n",
      "Epoch 4/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 6.8709e-04 - val_loss: 6.6112e-04\n",
      "Epoch 5/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 6.4939e-04 - val_loss: 6.2175e-04\n",
      "Epoch 6/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 6.2025e-04 - val_loss: 6.1320e-04\n",
      "Epoch 7/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.9659e-04 - val_loss: 6.1990e-04\n",
      "Epoch 8/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.8059e-04 - val_loss: 5.4456e-04\n",
      "Epoch 9/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.6386e-04 - val_loss: 5.4370e-04\n",
      "Epoch 10/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.5488e-04 - val_loss: 5.4029e-04\n",
      "Epoch 11/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.4270e-04 - val_loss: 5.3896e-04\n",
      "Epoch 12/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.3292e-04 - val_loss: 5.1237e-04\n",
      "Epoch 13/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.2931e-04 - val_loss: 5.5899e-04\n",
      "Epoch 14/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.2439e-04 - val_loss: 5.3172e-04\n",
      "Epoch 15/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.1794e-04 - val_loss: 5.0488e-04\n",
      "Epoch 16/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.1241e-04 - val_loss: 5.0715e-04\n",
      "Epoch 17/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.0440e-04 - val_loss: 4.8701e-04\n",
      "Epoch 18/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 5.0194e-04 - val_loss: 5.0477e-04\n",
      "Epoch 19/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.9327e-04 - val_loss: 4.8959e-04\n",
      "Epoch 20/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.9065e-04 - val_loss: 4.9182e-04\n",
      "Epoch 21/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.8768e-04 - val_loss: 5.0573e-04\n",
      "Epoch 22/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.8396e-04 - val_loss: 5.0988e-04\n",
      "Epoch 23/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.8057e-04 - val_loss: 4.8513e-04\n",
      "Epoch 24/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.7936e-04 - val_loss: 4.9950e-04\n",
      "Epoch 25/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.7161e-04 - val_loss: 5.1361e-04\n",
      "Epoch 26/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.6687e-04 - val_loss: 4.8631e-04\n",
      "Epoch 27/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.6416e-04 - val_loss: 4.5165e-04\n",
      "Epoch 28/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.6136e-04 - val_loss: 4.9277e-04\n",
      "Epoch 29/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.6320e-04 - val_loss: 4.7248e-04\n",
      "Epoch 30/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.5585e-04 - val_loss: 4.9051e-04\n",
      "Epoch 31/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.5386e-04 - val_loss: 4.5370e-04\n",
      "Epoch 32/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.5516e-04 - val_loss: 4.5142e-04\n",
      "Epoch 33/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.5086e-04 - val_loss: 4.5808e-04\n",
      "Epoch 34/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.4649e-04 - val_loss: 4.9976e-04\n",
      "Epoch 35/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.4860e-04 - val_loss: 4.6005e-04\n",
      "Epoch 36/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.4593e-04 - val_loss: 4.9699e-04\n",
      "Epoch 37/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.4517e-04 - val_loss: 4.6938e-04\n",
      "Epoch 38/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.4331e-04 - val_loss: 4.5040e-04\n",
      "Epoch 39/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3865e-04 - val_loss: 4.3410e-04\n",
      "Epoch 40/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3593e-04 - val_loss: 4.4411e-04\n",
      "Epoch 41/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3643e-04 - val_loss: 4.2810e-04\n",
      "Epoch 42/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3560e-04 - val_loss: 4.9014e-04\n",
      "Epoch 43/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3669e-04 - val_loss: 4.8370e-04\n",
      "Epoch 44/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3304e-04 - val_loss: 4.6498e-04\n",
      "Epoch 45/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3154e-04 - val_loss: 4.3293e-04\n",
      "Epoch 46/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2874e-04 - val_loss: 4.6170e-04\n",
      "Epoch 47/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.3103e-04 - val_loss: 4.7608e-04\n",
      "Epoch 48/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2838e-04 - val_loss: 4.6183e-04\n",
      "Epoch 49/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2604e-04 - val_loss: 4.5031e-04\n",
      "Epoch 50/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2589e-04 - val_loss: 4.5484e-04\n",
      "Epoch 51/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2269e-04 - val_loss: 4.2993e-04\n",
      "Epoch 52/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2354e-04 - val_loss: 4.2565e-04\n",
      "Epoch 53/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2145e-04 - val_loss: 4.4544e-04\n",
      "Epoch 54/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.2086e-04 - val_loss: 4.2578e-04\n",
      "Epoch 55/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1917e-04 - val_loss: 4.3225e-04\n",
      "Epoch 56/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1678e-04 - val_loss: 4.5516e-04\n",
      "Epoch 57/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1989e-04 - val_loss: 4.4447e-04\n",
      "Epoch 58/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1676e-04 - val_loss: 4.4762e-04\n",
      "Epoch 59/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1542e-04 - val_loss: 4.3508e-04\n",
      "Epoch 60/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1471e-04 - val_loss: 4.2954e-04\n",
      "Epoch 61/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1562e-04 - val_loss: 4.3957e-04\n",
      "Epoch 62/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1075e-04 - val_loss: 4.3384e-04\n",
      "Epoch 63/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.1464e-04 - val_loss: 4.0999e-04\n",
      "Epoch 64/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0983e-04 - val_loss: 4.2582e-04\n",
      "Epoch 65/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0762e-04 - val_loss: 4.3077e-04\n",
      "Epoch 66/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0679e-04 - val_loss: 4.1466e-04\n",
      "Epoch 67/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0840e-04 - val_loss: 4.5484e-04\n",
      "Epoch 68/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0652e-04 - val_loss: 4.4224e-04\n",
      "Epoch 69/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0472e-04 - val_loss: 4.7514e-04\n",
      "Epoch 70/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0402e-04 - val_loss: 4.1768e-04\n",
      "Epoch 71/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0182e-04 - val_loss: 4.5096e-04\n",
      "Epoch 72/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0238e-04 - val_loss: 4.1357e-04\n",
      "Epoch 73/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0088e-04 - val_loss: 4.0179e-04\n",
      "Epoch 74/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0010e-04 - val_loss: 4.2399e-04\n",
      "Epoch 75/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 4.0156e-04 - val_loss: 4.1347e-04\n",
      "Epoch 76/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9865e-04 - val_loss: 4.2878e-04\n",
      "Epoch 77/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9468e-04 - val_loss: 4.0910e-04\n",
      "Epoch 78/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9777e-04 - val_loss: 4.2720e-04\n",
      "Epoch 79/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9974e-04 - val_loss: 4.2428e-04\n",
      "Epoch 80/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9759e-04 - val_loss: 4.2103e-04\n",
      "Epoch 81/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9300e-04 - val_loss: 4.4711e-04\n",
      "Epoch 82/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9324e-04 - val_loss: 3.9456e-04\n",
      "Epoch 83/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9509e-04 - val_loss: 4.2518e-04\n",
      "Epoch 84/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9104e-04 - val_loss: 4.1786e-04\n",
      "Epoch 85/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9238e-04 - val_loss: 4.1725e-04\n",
      "Epoch 86/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9157e-04 - val_loss: 4.0453e-04\n",
      "Epoch 87/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8691e-04 - val_loss: 4.1863e-04\n",
      "Epoch 88/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8671e-04 - val_loss: 4.0853e-04\n",
      "Epoch 89/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8987e-04 - val_loss: 4.2967e-04\n",
      "Epoch 90/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8747e-04 - val_loss: 4.0156e-04\n",
      "Epoch 91/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9158e-04 - val_loss: 4.2158e-04\n",
      "Epoch 92/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9073e-04 - val_loss: 3.9738e-04\n",
      "Epoch 93/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.9078e-04 - val_loss: 4.2076e-04\n",
      "Epoch 94/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8839e-04 - val_loss: 4.0496e-04\n",
      "Epoch 95/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8486e-04 - val_loss: 4.1596e-04\n",
      "Epoch 96/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8868e-04 - val_loss: 4.0907e-04\n",
      "Epoch 97/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8670e-04 - val_loss: 4.1497e-04\n",
      "Epoch 98/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8322e-04 - val_loss: 4.2878e-04\n",
      "Epoch 99/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8338e-04 - val_loss: 4.2393e-04\n",
      "Epoch 100/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8471e-04 - val_loss: 4.1385e-04\n",
      "Epoch 101/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8153e-04 - val_loss: 3.9509e-04\n",
      "Epoch 102/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8320e-04 - val_loss: 3.9011e-04\n",
      "Epoch 103/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8055e-04 - val_loss: 4.0215e-04\n",
      "Epoch 104/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7906e-04 - val_loss: 4.0571e-04\n",
      "Epoch 105/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8254e-04 - val_loss: 4.1002e-04\n",
      "Epoch 106/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8082e-04 - val_loss: 4.3755e-04\n",
      "Epoch 107/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7885e-04 - val_loss: 4.1084e-04\n",
      "Epoch 108/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8053e-04 - val_loss: 3.9493e-04\n",
      "Epoch 109/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.8003e-04 - val_loss: 3.9485e-04\n",
      "Epoch 110/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7959e-04 - val_loss: 4.0468e-04\n",
      "Epoch 111/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7998e-04 - val_loss: 4.0651e-04\n",
      "Epoch 112/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7525e-04 - val_loss: 3.9520e-04\n",
      "Epoch 113/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7679e-04 - val_loss: 4.0975e-04\n",
      "Epoch 114/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7642e-04 - val_loss: 4.0530e-04\n",
      "Epoch 115/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7818e-04 - val_loss: 4.0726e-04\n",
      "Epoch 116/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7684e-04 - val_loss: 4.1084e-04\n",
      "Epoch 117/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7357e-04 - val_loss: 4.0109e-04\n",
      "Epoch 118/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7557e-04 - val_loss: 4.0895e-04\n",
      "Epoch 119/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7471e-04 - val_loss: 3.9928e-04\n",
      "Epoch 120/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7379e-04 - val_loss: 4.0986e-04\n",
      "Epoch 121/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7453e-04 - val_loss: 3.9038e-04\n",
      "Epoch 122/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7523e-04 - val_loss: 4.0141e-04\n",
      "Epoch 123/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7298e-04 - val_loss: 3.9598e-04\n",
      "Epoch 124/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7242e-04 - val_loss: 4.0942e-04\n",
      "Epoch 125/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7335e-04 - val_loss: 3.9454e-04\n",
      "Epoch 126/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7122e-04 - val_loss: 4.1937e-04\n",
      "Epoch 127/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7390e-04 - val_loss: 3.9376e-04\n",
      "Epoch 128/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6747e-04 - val_loss: 3.9822e-04\n",
      "Epoch 129/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7164e-04 - val_loss: 3.9986e-04\n",
      "Epoch 130/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7219e-04 - val_loss: 3.9083e-04\n",
      "Epoch 131/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7012e-04 - val_loss: 3.9488e-04\n",
      "Epoch 132/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.7106e-04 - val_loss: 4.0347e-04\n",
      "Epoch 133/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6839e-04 - val_loss: 3.9190e-04\n",
      "Epoch 134/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6985e-04 - val_loss: 3.8682e-04\n",
      "Epoch 135/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6589e-04 - val_loss: 3.8288e-04\n",
      "Epoch 136/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6915e-04 - val_loss: 3.8742e-04\n",
      "Epoch 137/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6829e-04 - val_loss: 3.9775e-04\n",
      "Epoch 138/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6907e-04 - val_loss: 3.9592e-04\n",
      "Epoch 139/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6891e-04 - val_loss: 4.0057e-04\n",
      "Epoch 140/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6728e-04 - val_loss: 3.9906e-04\n",
      "Epoch 141/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6959e-04 - val_loss: 3.8642e-04\n",
      "Epoch 142/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6651e-04 - val_loss: 3.8594e-04\n",
      "Epoch 143/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6481e-04 - val_loss: 3.8434e-04\n",
      "Epoch 144/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6529e-04 - val_loss: 3.9065e-04\n",
      "Epoch 145/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6838e-04 - val_loss: 3.9257e-04\n",
      "Epoch 146/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6536e-04 - val_loss: 3.9838e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6385e-04 - val_loss: 3.8585e-04\n",
      "Epoch 148/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6442e-04 - val_loss: 3.8908e-04\n",
      "Epoch 149/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6486e-04 - val_loss: 3.8396e-04\n",
      "Epoch 150/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6617e-04 - val_loss: 3.7645e-04\n",
      "Epoch 151/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6110e-04 - val_loss: 3.9948e-04\n",
      "Epoch 152/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6584e-04 - val_loss: 3.9375e-04\n",
      "Epoch 153/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6545e-04 - val_loss: 4.0158e-04\n",
      "Epoch 154/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6367e-04 - val_loss: 3.9663e-04\n",
      "Epoch 155/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6433e-04 - val_loss: 3.8657e-04\n",
      "Epoch 156/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6075e-04 - val_loss: 4.1503e-04\n",
      "Epoch 157/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6086e-04 - val_loss: 3.9762e-04\n",
      "Epoch 158/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6001e-04 - val_loss: 3.6932e-04\n",
      "Epoch 159/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6133e-04 - val_loss: 3.9145e-04\n",
      "Epoch 160/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6279e-04 - val_loss: 3.8207e-04\n",
      "Epoch 161/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6120e-04 - val_loss: 3.7186e-04\n",
      "Epoch 162/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5918e-04 - val_loss: 3.9482e-04\n",
      "Epoch 163/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6170e-04 - val_loss: 3.9319e-04\n",
      "Epoch 164/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6357e-04 - val_loss: 3.8381e-04\n",
      "Epoch 165/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5525e-04 - val_loss: 3.8592e-04\n",
      "Epoch 166/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6256e-04 - val_loss: 4.0371e-04\n",
      "Epoch 167/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5973e-04 - val_loss: 3.8832e-04\n",
      "Epoch 168/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6166e-04 - val_loss: 3.9648e-04\n",
      "Epoch 169/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5720e-04 - val_loss: 3.9194e-04\n",
      "Epoch 170/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6189e-04 - val_loss: 3.8509e-04\n",
      "Epoch 171/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5893e-04 - val_loss: 3.9823e-04\n",
      "Epoch 172/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.6030e-04 - val_loss: 3.7069e-04\n",
      "Epoch 173/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5850e-04 - val_loss: 3.7584e-04\n",
      "Epoch 174/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5897e-04 - val_loss: 3.7879e-04\n",
      "Epoch 175/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5968e-04 - val_loss: 4.0253e-04\n",
      "Epoch 176/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5748e-04 - val_loss: 3.7703e-04\n",
      "Epoch 177/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5498e-04 - val_loss: 3.8326e-04\n",
      "Epoch 178/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5997e-04 - val_loss: 3.9775e-04\n",
      "Epoch 179/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5739e-04 - val_loss: 3.8107e-04\n",
      "Epoch 180/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5423e-04 - val_loss: 3.7992e-04\n",
      "Epoch 181/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5697e-04 - val_loss: 3.8181e-04\n",
      "Epoch 182/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5851e-04 - val_loss: 3.8629e-04\n",
      "Epoch 183/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5616e-04 - val_loss: 3.9608e-04\n",
      "Epoch 184/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5855e-04 - val_loss: 3.7317e-04\n",
      "Epoch 185/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5376e-04 - val_loss: 3.7880e-04\n",
      "Epoch 186/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5673e-04 - val_loss: 3.7053e-04\n",
      "Epoch 187/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5613e-04 - val_loss: 3.7187e-04\n",
      "Epoch 188/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5387e-04 - val_loss: 3.8024e-04\n",
      "Epoch 189/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5802e-04 - val_loss: 3.7945e-04\n",
      "Epoch 190/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5662e-04 - val_loss: 4.0070e-04\n",
      "Epoch 191/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5627e-04 - val_loss: 3.7248e-04\n",
      "Epoch 192/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5773e-04 - val_loss: 3.6848e-04\n",
      "Epoch 193/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5493e-04 - val_loss: 3.7833e-04\n",
      "Epoch 194/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5538e-04 - val_loss: 3.8319e-04\n",
      "Epoch 195/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5431e-04 - val_loss: 3.9365e-04\n",
      "Epoch 196/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5247e-04 - val_loss: 3.7731e-04\n",
      "Epoch 197/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5492e-04 - val_loss: 3.5912e-04\n",
      "Epoch 198/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5179e-04 - val_loss: 3.9369e-04\n",
      "Epoch 199/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5470e-04 - val_loss: 3.7545e-04\n",
      "Epoch 200/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 3.5418e-04 - val_loss: 3.9421e-04\n"
     ]
    }
   ],
   "source": [
    "num_epochs=params['num_epochs']\n",
    "training_dir = 'training_dir'\n",
    "testcase = 'padding_unet'\n",
    "# Path(f'{training_dir}/tensorboard/{testcase}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.ModelCheckpoint(training_dir+f'/{testcase}.hdf5', \n",
    "                    monitor='val_loss',save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'{training_dir}/tensorboard/{testcase}')\n",
    "]\n",
    "\n",
    "history0 = unet.fit(train_data, validation_data=test_data,\n",
    "                  epochs=num_epochs,\n",
    "                  callbacks=callbacks,\n",
    "                  workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108e6cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b467fe17590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history0.history['loss'],label='Train loss')\n",
    "plt.plot(history0.history['val_loss'],label='Val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99c2a6",
   "metadata": {},
   "source": [
    "### Training without weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d392e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 11:34:47.233265: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 11:34:47.233324: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 11:34:47.233363: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 11:34:47.233494: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 11:34:47.233510: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   3/1313 [..............................] - ETA: 2:11 - loss: 0.7040 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 11:34:48.341526: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 11:34:48.341577: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 11:34:48.341665: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 11:34:48.465230: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-24 11:34:48.465341: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 11:34:48.468148: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-02-24 11:34:48.469072: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 11:34:48.475434: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48\n",
      "\n",
      "2022-02-24 11:34:48.477784: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.trace.json.gz\n",
      "2022-02-24 11:34:48.489014: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48\n",
      "\n",
      "2022-02-24 11:34:48.493631: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.memory_profile.json.gz\n",
      "2022-02-24 11:34:48.501650: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48\n",
      "Dumped tool data for xplane.pb to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.xplane.pb\n",
      "Dumped tool data for overview_page.pb to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to training_dir/tensorboard/padding_unet_batch8/train/plugins/profile/2022_02_24_11_34_48/d3103.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0641 - val_loss: 0.0391\n",
      "Epoch 2/200\n",
      "1313/1313 [==============================] - 9s 6ms/step - loss: 0.0373 - val_loss: 0.0363\n",
      "Epoch 3/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0342 - val_loss: 0.0332\n",
      "Epoch 4/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0319 - val_loss: 0.0368\n",
      "Epoch 5/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0306 - val_loss: 0.0312\n",
      "Epoch 6/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0294 - val_loss: 0.0283\n",
      "Epoch 7/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0283 - val_loss: 0.0280\n",
      "Epoch 8/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0273 - val_loss: 0.0307\n",
      "Epoch 9/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0271 - val_loss: 0.0260\n",
      "Epoch 10/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0262 - val_loss: 0.0273\n",
      "Epoch 11/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0258 - val_loss: 0.0263\n",
      "Epoch 12/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0252 - val_loss: 0.0257\n",
      "Epoch 13/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0251 - val_loss: 0.0265\n",
      "Epoch 14/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0247 - val_loss: 0.0251\n",
      "Epoch 15/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0245 - val_loss: 0.0269\n",
      "Epoch 16/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 17/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0238 - val_loss: 0.0243\n",
      "Epoch 18/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0236 - val_loss: 0.0243\n",
      "Epoch 19/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0236 - val_loss: 0.0234\n",
      "Epoch 20/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 21/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0232 - val_loss: 0.0255\n",
      "Epoch 22/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0231 - val_loss: 0.0239\n",
      "Epoch 23/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0229 - val_loss: 0.0251\n",
      "Epoch 24/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0227 - val_loss: 0.0227\n",
      "Epoch 25/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 26/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0225 - val_loss: 0.0230\n",
      "Epoch 27/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0222 - val_loss: 0.0229\n",
      "Epoch 28/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 29/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0220 - val_loss: 0.0246\n",
      "Epoch 30/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 31/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0220 - val_loss: 0.0237\n",
      "Epoch 32/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0219 - val_loss: 0.0231\n",
      "Epoch 33/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0216 - val_loss: 0.0250\n",
      "Epoch 34/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0216 - val_loss: 0.0236\n",
      "Epoch 35/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0215 - val_loss: 0.0218\n",
      "Epoch 36/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0213 - val_loss: 0.0224\n",
      "Epoch 37/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0214 - val_loss: 0.0246\n",
      "Epoch 38/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0212 - val_loss: 0.0231\n",
      "Epoch 39/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0211 - val_loss: 0.0260\n",
      "Epoch 40/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0209 - val_loss: 0.0264\n",
      "Epoch 41/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0208 - val_loss: 0.0243\n",
      "Epoch 42/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0207 - val_loss: 0.0252\n",
      "Epoch 43/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0208 - val_loss: 0.0229\n",
      "Epoch 44/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0207 - val_loss: 0.0248\n",
      "Epoch 45/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 46/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0245\n",
      "Epoch 47/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0204 - val_loss: 0.0214\n",
      "Epoch 48/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0245\n",
      "Epoch 49/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0203 - val_loss: 0.0240\n",
      "Epoch 50/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0201 - val_loss: 0.0212\n",
      "Epoch 51/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0227\n",
      "Epoch 52/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0200 - val_loss: 0.0211\n",
      "Epoch 53/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0211\n",
      "Epoch 54/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0226\n",
      "Epoch 55/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0220\n",
      "Epoch 56/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0198 - val_loss: 0.0215\n",
      "Epoch 57/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0206\n",
      "Epoch 58/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0263\n",
      "Epoch 59/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 60/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0238\n",
      "Epoch 61/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0196 - val_loss: 0.0237\n",
      "Epoch 62/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0210\n",
      "Epoch 63/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0194 - val_loss: 0.0215\n",
      "Epoch 64/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0196 - val_loss: 0.0218\n",
      "Epoch 65/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0218\n",
      "Epoch 66/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0194 - val_loss: 0.0219\n",
      "Epoch 67/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0193 - val_loss: 0.0229\n",
      "Epoch 68/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0219\n",
      "Epoch 69/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0264\n",
      "Epoch 70/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0193 - val_loss: 0.0212\n",
      "Epoch 71/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0210\n",
      "Epoch 72/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0211\n",
      "Epoch 73/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0204\n",
      "Epoch 74/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0204\n",
      "Epoch 75/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0205\n",
      "Epoch 76/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0225\n",
      "Epoch 77/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0225\n",
      "Epoch 78/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0219\n",
      "Epoch 79/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 81/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0244\n",
      "Epoch 82/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 83/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0187 - val_loss: 0.0225\n",
      "Epoch 84/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 85/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 86/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "Epoch 87/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 88/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 89/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0185 - val_loss: 0.0204\n",
      "Epoch 90/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0185 - val_loss: 0.0213\n",
      "Epoch 91/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0186 - val_loss: 0.0215\n",
      "Epoch 92/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0184 - val_loss: 0.0203\n",
      "Epoch 93/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0185 - val_loss: 0.0206\n",
      "Epoch 94/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0197\n",
      "Epoch 95/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0184 - val_loss: 0.0200\n",
      "Epoch 96/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 97/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0204\n",
      "Epoch 98/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0182 - val_loss: 0.0233\n",
      "Epoch 99/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0202\n",
      "Epoch 100/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0219\n",
      "Epoch 101/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0208\n",
      "Epoch 102/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 103/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0205\n",
      "Epoch 104/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0219\n",
      "Epoch 105/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0201\n",
      "Epoch 106/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0210\n",
      "Epoch 107/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0180 - val_loss: 0.0191\n",
      "Epoch 108/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0207\n",
      "Epoch 109/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0182 - val_loss: 0.0199\n",
      "Epoch 110/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0201\n",
      "Epoch 111/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0221\n",
      "Epoch 112/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0182 - val_loss: 0.0203\n",
      "Epoch 113/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0199\n",
      "Epoch 114/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0180 - val_loss: 0.0198\n",
      "Epoch 115/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0180 - val_loss: 0.0203\n",
      "Epoch 116/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0207\n",
      "Epoch 117/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0180 - val_loss: 0.0188\n",
      "Epoch 118/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0201\n",
      "Epoch 119/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0196\n",
      "Epoch 120/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0196\n",
      "Epoch 121/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0181 - val_loss: 0.0208\n",
      "Epoch 122/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0204\n",
      "Epoch 123/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0179 - val_loss: 0.0203\n",
      "Epoch 124/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0178 - val_loss: 0.0198\n",
      "Epoch 125/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0177 - val_loss: 0.0230\n",
      "Epoch 126/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0178 - val_loss: 0.0197\n",
      "Epoch 127/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0178 - val_loss: 0.0194\n",
      "Epoch 128/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0178 - val_loss: 0.0223\n",
      "Epoch 129/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0177 - val_loss: 0.0195\n",
      "Epoch 130/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0219\n",
      "Epoch 131/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0178 - val_loss: 0.0194\n",
      "Epoch 132/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0222\n",
      "Epoch 133/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0197\n",
      "Epoch 134/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0177 - val_loss: 0.0195\n",
      "Epoch 135/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0195\n",
      "Epoch 136/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0177 - val_loss: 0.0216\n",
      "Epoch 137/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0175 - val_loss: 0.0193\n",
      "Epoch 138/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0212\n",
      "Epoch 139/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0235\n",
      "Epoch 140/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0194\n",
      "Epoch 141/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0175 - val_loss: 0.0192\n",
      "Epoch 142/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0203\n",
      "Epoch 143/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0175 - val_loss: 0.0227\n",
      "Epoch 144/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0175 - val_loss: 0.0203\n",
      "Epoch 145/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0176 - val_loss: 0.0194\n",
      "Epoch 146/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0201\n",
      "Epoch 147/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0174 - val_loss: 0.0194\n",
      "Epoch 148/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0204\n",
      "Epoch 149/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0174 - val_loss: 0.0192\n",
      "Epoch 150/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0195\n",
      "Epoch 151/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 152/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0208\n",
      "Epoch 153/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0174 - val_loss: 0.0214\n",
      "Epoch 154/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0198\n",
      "Epoch 155/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0203\n",
      "Epoch 156/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0172 - val_loss: 0.0195\n",
      "Epoch 157/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0212\n",
      "Epoch 158/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0189\n",
      "Epoch 159/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0200\n",
      "Epoch 160/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0174 - val_loss: 0.0205\n",
      "Epoch 161/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0208\n",
      "Epoch 162/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0193\n",
      "Epoch 163/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0172 - val_loss: 0.0196\n",
      "Epoch 164/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0172 - val_loss: 0.0185\n",
      "Epoch 165/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0189\n",
      "Epoch 166/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0205\n",
      "Epoch 167/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0183\n",
      "Epoch 168/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0172 - val_loss: 0.0185\n",
      "Epoch 169/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0191\n",
      "Epoch 170/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0194\n",
      "Epoch 171/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0199\n",
      "Epoch 172/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0199\n",
      "Epoch 173/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0212\n",
      "Epoch 174/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0200\n",
      "Epoch 175/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0197\n",
      "Epoch 176/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0193\n",
      "Epoch 177/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0225\n",
      "Epoch 178/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0192\n",
      "Epoch 179/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0200\n",
      "Epoch 180/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0204\n",
      "Epoch 181/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0192\n",
      "Epoch 182/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0171 - val_loss: 0.0206\n",
      "Epoch 183/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0197\n",
      "Epoch 184/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0212\n",
      "Epoch 185/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0184\n",
      "Epoch 186/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0209\n",
      "Epoch 187/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0195\n",
      "Epoch 188/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0192\n",
      "Epoch 189/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0170 - val_loss: 0.0191\n",
      "Epoch 190/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0191\n",
      "Epoch 191/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0193\n",
      "Epoch 192/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0191\n",
      "Epoch 193/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0167 - val_loss: 0.0193\n",
      "Epoch 194/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0210\n",
      "Epoch 195/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0184\n",
      "Epoch 196/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0187\n",
      "Epoch 197/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0197\n",
      "Epoch 198/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0184\n",
      "Epoch 199/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0189\n",
      "Epoch 200/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0200\n"
     ]
    }
   ],
   "source": [
    "unet = padding_model() \n",
    "params={\n",
    "    'start_neurons'   :8,      # Controls size of hidden layers in CNN, higher = more complexity \n",
    "    'activation'      :'relu',  # Activation used throughout the U-Net,  see https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "    'loss'            :'mae',   # Either 'mae' or 'mse', or others as https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "    'loss_weights'    : 1,    # Scale for loss.  Recommend squaring this if using MSE\n",
    "    'opt'             :tf.keras.optimizers.Adam,  # optimizer, see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    'learning_rate'   :0.001,   # Learning rate for optimizer\n",
    "    'num_epochs'      :200,       # Number of epochs to train for\n",
    "    'batch'           :8\n",
    "}\n",
    "opt=params['opt'](learning_rate=params['learning_rate'])\n",
    "unet.compile(optimizer=opt, loss=params['loss'])\n",
    "\n",
    "training_dir = 'training_dir'\n",
    "testcase = 'padding_unet_batch8'\n",
    "# Path(f'{training_dir}/tensorboard/{testcase}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.ModelCheckpoint(training_dir+f'/{testcase}.hdf5', \n",
    "                    monitor='val_loss',save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'{training_dir}/tensorboard/{testcase}')\n",
    "]\n",
    "\n",
    "history1 = unet.fit(train_data, validation_data=test_data,\n",
    "                  epochs=params['num_epochs'],\n",
    "                  callbacks=callbacks,\n",
    "                  workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9efcc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b4561f29f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['loss'],label='Train loss')\n",
    "plt.plot(history1.history['val_loss'],label='Val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4c67a",
   "metadata": {},
   "source": [
    "### Training with mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7999e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:06:59.480291: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 12:06:59.480345: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 12:06:59.480561: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:06:59.480632: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 12:06:59.480653: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   3/1313 [..............................] - ETA: 2:03 - loss: 1.9584 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:07:00.451752: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 12:07:00.451805: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 12:07:00.451890: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:07:00.563123: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-24 12:07:00.563244: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 12:07:00.566034: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-02-24 12:07:00.567013: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 12:07:00.573951: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00\n",
      "\n",
      "2022-02-24 12:07:00.576460: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.trace.json.gz\n",
      "2022-02-24 12:07:00.587902: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00\n",
      "\n",
      "2022-02-24 12:07:00.592481: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.memory_profile.json.gz\n",
      "2022-02-24 12:07:00.602032: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00\n",
      "Dumped tool data for xplane.pb to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.xplane.pb\n",
      "Dumped tool data for overview_page.pb to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to training_dir/tensorboard/padding_unet_batch8_mse/train/plugins/profile/2022_02_24_12_07_00/d3103.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 9s 6ms/step - loss: 0.0358 - val_loss: 0.0050\n",
      "Epoch 2/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 3/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 4/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 5/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 6/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 7/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 8/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 9/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 10/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 11/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 12/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 13/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 14/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 15/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 16/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 17/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 18/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 19/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 20/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 21/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 22/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 23/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 24/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 25/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 26/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 27/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 28/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 29/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 30/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 31/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 32/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 33/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 34/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 35/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 36/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 37/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 38/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 39/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 40/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 41/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 42/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 43/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 44/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 45/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 46/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 47/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 48/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 49/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 50/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 51/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 52/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 53/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 54/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 55/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 56/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 57/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 58/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 59/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 60/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 61/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 62/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 63/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 64/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 65/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 66/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 67/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 68/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 69/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 70/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 71/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 72/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 73/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 74/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 75/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 76/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 77/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 78/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 79/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 81/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 82/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 83/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 84/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 85/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 86/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 87/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 88/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 89/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 90/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 91/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 92/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 93/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 94/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 95/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 96/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 97/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 98/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 99/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 100/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 101/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 102/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 103/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 104/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 105/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 106/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 107/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 108/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 109/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 110/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 111/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 112/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 113/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 114/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 115/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 116/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 117/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 118/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 119/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 120/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 121/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 122/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 123/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 124/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 125/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 126/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 127/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 128/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 129/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 130/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 131/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 132/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 133/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 134/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 135/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 136/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 137/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 138/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 139/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 140/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 141/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 142/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 143/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 144/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 145/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 146/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 147/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 148/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 149/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 150/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 151/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 152/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 153/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 154/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 155/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 156/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 157/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 158/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 159/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 160/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 161/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 162/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 163/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 164/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 165/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 166/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 167/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 168/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 169/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 170/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 171/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 172/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 173/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 174/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 175/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 176/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 177/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 178/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 179/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 180/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 181/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 182/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 183/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 184/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 185/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 186/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 187/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 188/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 189/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 190/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 191/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 192/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 193/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 194/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 195/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 196/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 197/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 198/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 199/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 200/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "unet = padding_model() \n",
    "params={\n",
    "    'start_neurons'   :8,      # Controls size of hidden layers in CNN, higher = more complexity \n",
    "    'activation'      :'relu',  # Activation used throughout the U-Net,  see https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "    'loss'            :'mse',   # Either 'mae' or 'mse', or others as https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "    'loss_weights'    : 1,    # Scale for loss.  Recommend squaring this if using MSE\n",
    "    'opt'             :tf.keras.optimizers.Adam,  # optimizer, see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    'learning_rate'   :0.001,   # Learning rate for optimizer\n",
    "    'num_epochs'      :200,       # Number of epochs to train for\n",
    "    'batch'           :8\n",
    "}\n",
    "opt=params['opt'](learning_rate=params['learning_rate'])\n",
    "unet.compile(optimizer=opt, loss=params['loss'])\n",
    "\n",
    "training_dir = 'training_dir'\n",
    "testcase = 'padding_unet_batch8_mse'\n",
    "# Path(f'{training_dir}/tensorboard/{testcase}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.ModelCheckpoint(training_dir+f'/{testcase}.hdf5', \n",
    "                    monitor='val_loss',save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'{training_dir}/tensorboard/{testcase}')\n",
    "]\n",
    "\n",
    "history2 = unet.fit(train_data, validation_data=test_data,\n",
    "                  epochs=params['num_epochs'],\n",
    "                  callbacks=callbacks,\n",
    "                  workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a29eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b45e4c6ed10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history2.history['loss'],label='Train loss')\n",
    "plt.plot(history2.history['val_loss'],label='Val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aa821",
   "metadata": {},
   "source": [
    "### Training with z-score normalized input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8956e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_data)\n",
    "var = np.var(X_data)\n",
    "\n",
    "norm_X = (X_data - mean) / math.sqrt(var)\n",
    "\n",
    "normslice = tf.data.Dataset.from_tensor_slices((norm_X,Y_data)).shuffle(buffer_size=len(Y_data),reshuffle_each_iteration=True).batch(params['batch'])\n",
    "train_norm = normslice.skip(int(len(normslice) * 0.25))\n",
    "test_norm = normslice.take(int(len(normslice) * 0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bb381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 18:28:07.116869: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 18:28:07.116918: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 18:28:07.116952: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2022-02-24 18:28:07.122708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.3/lib64:/shared/centos7/openmpi/4.0.2/lib:/shared/centos7/gcc/6.4.0/lib64:/shared/centos7/gcc/6.4.0/lib64/gcj-6.4.0-17:/shared/centos7/gcc/6.4.0/libexec/gcc/x86_64-pc-linux-gnu/6.4.0:/shared/centos7/gcc/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib:/shared/centos7/oracle_java/jdk1.8.0_181/lib:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64/jli:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64/server:/shared/centos7/oracle_java/jdk1.8.0_181/lib/amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/amd64/jli:/shared/centos7/oracle_java/jdk1.8.0_181/lib/visualvm/profiler/lib/deployed/jdk16/linux-amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/visualvm/profiler/lib/deployed/jdk15/linux-amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/missioncontrol/plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20141007-2033\n",
      "2022-02-24 18:28:07.127470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.3/lib64:/shared/centos7/openmpi/4.0.2/lib:/shared/centos7/gcc/6.4.0/lib64:/shared/centos7/gcc/6.4.0/lib64/gcj-6.4.0-17:/shared/centos7/gcc/6.4.0/libexec/gcc/x86_64-pc-linux-gnu/6.4.0:/shared/centos7/gcc/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib:/shared/centos7/oracle_java/jdk1.8.0_181/lib:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64/jli:/shared/centos7/oracle_java/jdk1.8.0_181/jre/lib/amd64/server:/shared/centos7/oracle_java/jdk1.8.0_181/lib/amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/amd64/jli:/shared/centos7/oracle_java/jdk1.8.0_181/lib/visualvm/profiler/lib/deployed/jdk16/linux-amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/visualvm/profiler/lib/deployed/jdk15/linux-amd64:/shared/centos7/oracle_java/jdk1.8.0_181/lib/missioncontrol/plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20141007-2033\n",
      "2022-02-24 18:28:07.127492: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 18:28:07.127511: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 18:28:07.127532: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 18:28:07.850274: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-24 18:28:08.835483: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  22/1313 [..............................] - ETA: 12s - loss: 0.7051 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 18:28:11.179351: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-24 18:28:11.179401: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-24 18:28:11.179445: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 18:28:11.189149: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-24 18:28:11.189255: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-02-24 18:28:11.193480: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-02-24 18:28:11.194961: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-24 18:28:11.201638: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11\n",
      "\n",
      "2022-02-24 18:28:11.203835: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.trace.json.gz\n",
      "2022-02-24 18:28:11.218676: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11\n",
      "\n",
      "2022-02-24 18:28:11.222961: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.memory_profile.json.gz\n",
      "2022-02-24 18:28:11.230887: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11\n",
      "Dumped tool data for xplane.pb to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.xplane.pb\n",
      "Dumped tool data for overview_page.pb to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to training_dir/tensorboard/norm_unet_batch8/train/plugins/profile/2022_02_24_18_28_11/d3103.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 12s 6ms/step - loss: 0.0888 - val_loss: 0.0427\n",
      "Epoch 2/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0394 - val_loss: 0.0358\n",
      "Epoch 3/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0345 - val_loss: 0.0324\n",
      "Epoch 4/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0322 - val_loss: 0.0300\n",
      "Epoch 5/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0304 - val_loss: 0.0294\n",
      "Epoch 6/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0294 - val_loss: 0.0291\n",
      "Epoch 7/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0285 - val_loss: 0.0275\n",
      "Epoch 8/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0277 - val_loss: 0.0271\n",
      "Epoch 9/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0268 - val_loss: 0.0276\n",
      "Epoch 10/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0263 - val_loss: 0.0267\n",
      "Epoch 11/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0257 - val_loss: 0.0262\n",
      "Epoch 12/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0255 - val_loss: 0.0259\n",
      "Epoch 13/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0251 - val_loss: 0.0280\n",
      "Epoch 14/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0249 - val_loss: 0.0247\n",
      "Epoch 15/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 16/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0243 - val_loss: 0.0261\n",
      "Epoch 17/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0239 - val_loss: 0.0261\n",
      "Epoch 18/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0238 - val_loss: 0.0253\n",
      "Epoch 19/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0238 - val_loss: 0.0237\n",
      "Epoch 20/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 21/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 22/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0231 - val_loss: 0.0234\n",
      "Epoch 23/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0229 - val_loss: 0.0235\n",
      "Epoch 24/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 25/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0227 - val_loss: 0.0231\n",
      "Epoch 26/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 27/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0224 - val_loss: 0.0228\n",
      "Epoch 28/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0224 - val_loss: 0.0231\n",
      "Epoch 29/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 30/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0221 - val_loss: 0.0230\n",
      "Epoch 31/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 32/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "Epoch 33/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 34/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0217 - val_loss: 0.0221\n",
      "Epoch 35/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0216 - val_loss: 0.0223\n",
      "Epoch 36/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0215 - val_loss: 0.0220\n",
      "Epoch 37/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0216 - val_loss: 0.0234\n",
      "Epoch 38/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0213 - val_loss: 0.0216\n",
      "Epoch 39/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0212 - val_loss: 0.0228\n",
      "Epoch 40/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0210 - val_loss: 0.0237\n",
      "Epoch 41/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0210 - val_loss: 0.0227\n",
      "Epoch 42/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0211 - val_loss: 0.0210\n",
      "Epoch 43/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0209 - val_loss: 0.0218\n",
      "Epoch 44/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0208 - val_loss: 0.0208\n",
      "Epoch 45/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0208 - val_loss: 0.0225\n",
      "Epoch 46/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 47/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0208 - val_loss: 0.0224\n",
      "Epoch 48/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0206 - val_loss: 0.0228\n",
      "Epoch 49/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0213\n",
      "Epoch 50/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 51/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0225\n",
      "Epoch 52/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0208\n",
      "Epoch 53/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 54/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0203 - val_loss: 0.0207\n",
      "Epoch 55/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0213\n",
      "Epoch 56/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0204 - val_loss: 0.0211\n",
      "Epoch 57/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0203 - val_loss: 0.0222\n",
      "Epoch 58/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0210\n",
      "Epoch 59/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0222\n",
      "Epoch 60/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0202 - val_loss: 0.0208\n",
      "Epoch 61/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0201 - val_loss: 0.0219\n",
      "Epoch 62/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 63/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0204\n",
      "Epoch 64/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0199 - val_loss: 0.0215\n",
      "Epoch 65/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0200 - val_loss: 0.0204\n",
      "Epoch 66/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0198 - val_loss: 0.0215\n",
      "Epoch 67/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0200 - val_loss: 0.0208\n",
      "Epoch 68/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0198 - val_loss: 0.0199\n",
      "Epoch 69/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0203\n",
      "Epoch 70/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0198 - val_loss: 0.0222\n",
      "Epoch 71/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 72/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0204\n",
      "Epoch 73/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0197 - val_loss: 0.0211\n",
      "Epoch 74/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0196 - val_loss: 0.0213\n",
      "Epoch 75/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0194 - val_loss: 0.0204\n",
      "Epoch 76/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0195 - val_loss: 0.0226\n",
      "Epoch 77/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0195 - val_loss: 0.0212\n",
      "Epoch 78/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0194 - val_loss: 0.0204\n",
      "Epoch 79/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0195 - val_loss: 0.0209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0195 - val_loss: 0.0206\n",
      "Epoch 81/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0194 - val_loss: 0.0201\n",
      "Epoch 82/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0193 - val_loss: 0.0210\n",
      "Epoch 83/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0195 - val_loss: 0.0202\n",
      "Epoch 84/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0193 - val_loss: 0.0210\n",
      "Epoch 85/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0211\n",
      "Epoch 86/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0193 - val_loss: 0.0207\n",
      "Epoch 87/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0204\n",
      "Epoch 88/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0211\n",
      "Epoch 89/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0207\n",
      "Epoch 90/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0202\n",
      "Epoch 91/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0204\n",
      "Epoch 92/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0207\n",
      "Epoch 93/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0216\n",
      "Epoch 94/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0192 - val_loss: 0.0201\n",
      "Epoch 95/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0212\n",
      "Epoch 96/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0191 - val_loss: 0.0198\n",
      "Epoch 97/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0203\n",
      "Epoch 98/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0205\n",
      "Epoch 99/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0211\n",
      "Epoch 100/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0199\n",
      "Epoch 101/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0204\n",
      "Epoch 102/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0224\n",
      "Epoch 103/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0205\n",
      "Epoch 104/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0210\n",
      "Epoch 105/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0208\n",
      "Epoch 106/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 107/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 108/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0186 - val_loss: 0.0207\n",
      "Epoch 109/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 110/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0187 - val_loss: 0.0211\n",
      "Epoch 111/200\n",
      "1313/1313 [==============================] - 8s 6ms/step - loss: 0.0187 - val_loss: 0.0209\n",
      "Epoch 112/200\n",
      "1075/1313 [=======================>......] - ETA: 1s - loss: 0.0186"
     ]
    }
   ],
   "source": [
    "unet = padding_model() \n",
    "params={\n",
    "    'start_neurons'   :8,      # Controls size of hidden layers in CNN, higher = more complexity \n",
    "    'activation'      :'relu',  # Activation used throughout the U-Net,  see https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "    'loss'            :'mae',   # Either 'mae' or 'mse', or others as https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "    'loss_weights'    : 1,    # Scale for loss.  Recommend squaring this if using MSE\n",
    "    'opt'             :tf.keras.optimizers.Adam,  # optimizer, see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    'learning_rate'   :0.001,   # Learning rate for optimizer\n",
    "    'num_epochs'      :200,       # Number of epochs to train for\n",
    "    'batch'           :8\n",
    "}\n",
    "opt=params['opt'](learning_rate=params['learning_rate'])\n",
    "unet.compile(optimizer=opt, loss=params['loss'])\n",
    "\n",
    "training_dir = 'training_dir'\n",
    "testcase = 'norm_unet_batch8'\n",
    "# Path(f'{training_dir}/tensorboard/{testcase}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.ModelCheckpoint(training_dir+f'/{testcase}.hdf5', \n",
    "                    monitor='val_loss',save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'{training_dir}/tensorboard/{testcase}')\n",
    "]\n",
    "\n",
    "history3 = unet.fit(train_norm, validation_data=test_norm,\n",
    "                  epochs=params['num_epochs'],\n",
    "                  callbacks=callbacks,\n",
    "                  workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c69543",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history['loss'],label='Train loss')\n",
    "plt.plot(history3.history['val_loss'],label='Val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0509d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(X_data[5000], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = unet.predict(np.expand_dims(X_data[5000], axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
